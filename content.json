{"meta":{"title":"岳岳博客","subtitle":null,"description":null,"author":"yueyq2017","url":"https://yongqiangyue.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2019-07-10T10:15:48.973Z","updated":"2019-07-10T06:44:03.000Z","comments":false,"path":"/404.html","permalink":"https://yongqiangyue.github.io//404.html","excerpt":"","text":""},{"title":"分类","date":"2019-07-10T09:18:11.000Z","updated":"2019-07-10T09:50:00.546Z","comments":true,"path":"categories/index.html","permalink":"https://yongqiangyue.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-07-10T09:17:24.000Z","updated":"2019-07-10T09:50:00.544Z","comments":false,"path":"tags/index.html","permalink":"https://yongqiangyue.github.io/tags/index.html","excerpt":"","text":""},{"title":"Repository","date":"2019-07-10T09:19:29.000Z","updated":"2019-07-10T09:49:31.352Z","comments":false,"path":"repository/index.html","permalink":"https://yongqiangyue.github.io/repository/index.html","excerpt":"","text":""},{"title":"关于","date":"2019-07-10T09:47:49.000Z","updated":"2019-07-10T09:49:21.099Z","comments":true,"path":"about/index.html","permalink":"https://yongqiangyue.github.io/about/index.html","excerpt":"","text":""}],"posts":[{"title":"深度学习-Faster R-CNN","slug":"深度学习-Faster-R-CNN-2","date":"2019-09-23T06:52:05.000Z","updated":"2020-05-26T08:29:23.089Z","comments":true,"path":"2019/09/23/深度学习-Faster-R-CNN-2/","link":"","permalink":"https://yongqiangyue.github.io/2019/09/23/深度学习-Faster-R-CNN-2/","excerpt":"","text":"读论文 debug code 核心点：RPN目标：能口述网络结构图，能实现nms算法，能实现IOU时间点：20190923 读第一遍论文，整理中间所有的知识点知识点1: 准确率和召回率，mAP准确率：认为是正样本中有多少被猜对了召回率：所有正样本中有多少被找出了了ground truth: 正确打标签的训练数据（x, label）,其中label是正确的标签AP（Average Precision）：类别AP是P-R曲线所围成下面积？Confidences:是怎么设计的？mAP(mean Average Precision):物体检测中是不同物体的AP的平均值知识点2：交并比IOU公式：$$IOU=\\frac{A\\cap B}{A \\cup B}$$知识点3：非极大值抑制NMSnms: Non-Maximum Suppression对于有重叠的候选区框：若大于规定阈值（某个提前设定的置信度？）则删除，低于阈值的保留。对于无重叠的候选框：都保留。代码实现过程： 获得当前目标类别下的所有的bbx信息 将bbx列表按照confidence从高到低排序，并记录当前confidence最大的bbx 计算最大confidence对应的bbx与剩下所有的bbx的IOU，移除所有大于阈值的bbx 对剩下的bbx，循环执行2, 3直到所有的bbx列表为空 知识点：R-CNN，Fast R-CNN, Faster R-CNN R-CNN: 选择候选框的方式：Selective Search.分类方式：每个物体进行20个SVM分类器；针对每个类，通过计算IOU，采用非极大抑制算法，以置信度最高的区域为基础，删除那些重叠位置的区域。（nms算法）","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://yongqiangyue.github.io/categories/深度学习/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://yongqiangyue.github.io/tags/hexo/"}]},{"title":"0728人工智能RNN","slug":"0728人工智能RNN","date":"2019-08-04T13:58:07.000Z","updated":"2019-08-04T13:58:07.356Z","comments":true,"path":"2019/08/04/0728人工智能RNN/","link":"","permalink":"https://yongqiangyue.github.io/2019/08/04/0728人工智能RNN/","excerpt":"","text":"课上总结循环神经网络RNN数据 CNN 一维连续数据(语音)：[BatchSize, Time, Feature] 1维数据（文本）：[BatchSize, Length, Channel] 二维数据（图像）：[BatchSize, Height, Width, Channel] 2维数据（图像）：[BatchSize, Channel, …] 数据实例 1维连续数据：波形数据（语音）、文本数据 2维连续数据：图像 无顺序文本模型 Bag of words:词袋 对比Embedding处理的单位是‘字’ 一句话：‘今天我的心情很好’ BAG：‘今天 我 的 心情 很好’：也有一定的顺序模型 建模 首先，需要考虑的是如何将数据提取为向量：数据向量化 embedding: onehot and 降维 第二，需要考虑的是这个向量携带的信息 复习：CNN(提取特征) 输入：[BatchSize, H, W, C] 核心：[KernelSize1, KernerSize2, C, C2] 概念：感受野 输出：[B, H, W,C2] 如何针对文本问题进行建模 思考：如何使用CNN完成长度为5的文本分类 输入：[B, 5, 5000] 对全连接网络进行改进 RNN：保证因果上的依赖关系 文本向量化：Embedding 输入：[B, Length]-&gt;int Embedding:[B, L, EmbeddingSize]-&gt;float RNN的改进模型：LSTM：长短时间记忆单元优化 如何进行梯度的计算：BP算法，BPTT算法 可以使用交叉熵等常见的损失函数 对应LSTM的损失函数引入sequence-loss KL散度：$D_{KL}(p||q)=\\sum_{i=1}^{n}{p(x_i)log(\\frac {p(x_i)}{q(x_i)})}$代码 Basic Tensor Plane Vector 维度 Embedding 文本向量化 tf.nn.embedding_lookup就是一个全连接神经网络 并且参数可以训练 argv: sys.argv rnn单元使用 隐藏状态$h_t=f(h_{t-1}, x_t)$ lstm/gru都是对RNN的改进 sequence-loss 文本分类 文本分词 文本生成","categories":[],"tags":[]},{"title":"数学公式","slug":"数学公式-6","date":"2019-08-01T02:50:48.000Z","updated":"2019-08-01T04:00:00.515Z","comments":true,"path":"2019/08/01/数学公式-6/","link":"","permalink":"https://yongqiangyue.github.io/2019/08/01/数学公式-6/","excerpt":"","text":"$$D(X)=E((X-\\bar{X})^2)=E(X^2)-E^2(X)$$$$\\bar{X}= \\frac{1}{n}\\sum_{i=1}^{n}{X_i}$$$$S^2=\\frac{1}{n-1}\\sum_{i=1}^n{(X_i-\\bar{X})^2}$$$$H(X)=-\\sum_{i=1}^n{p(x_i)logp(x_i)}$$$$H(X)=-\\sum_{i=1}^n{p(x_i)logq(x_i)}$$$$Cov(X,Y)=E((X-E(X))\\odot(Y-E(Y)))$$$$Ax=\\lambda x$$$$L=\\sum_{i=1}^{n}{(y_i-d_i)^2}$$$$f(x)=\\frac{1}{1+e^{-x}}$$ LSTM：长短期记忆网络：引入了三个控制门：it, ft, ot,ct 输入门：控制当前计算的新状态以多大程度更新到记忆单元中 输出门：控制当前输出有多大程度取决于当前到记忆单元 遗忘门：控制前一步记忆单元中到信息有多大程度被遗忘掉 内部记忆单元CNN和RNN的练习(522=20个练习) 手写数字识别，CNN，RNN（tensorflow, keras） 迁移学习 文本分类，CNN，RNN（tensorflow, keras） 文本分词，CNN，RNN，双向RNN（tensorflow, keras） 双向RNN：加入延迟 文本生成，CNN，RNN（tensorflow, keras） 课堂翻转，CNN，RNN（tensorflow, keras）推断过程很重要（文本生成） 训练过程和预测过程有区别 训练过程 输入：“曲项向天歌，白毛浮绿水” 标签：“相遇总相宜，向前不是梦” 模型：多层RNN模型 损失：Sequence-loss:序列损失函数 预测过程（*） 输入：首个字符 输入到RNN中获取输出 解码出文字：根据概率解码出下一个字符 贪心解码—&gt;每次结果相同：自然语言翻译 随机策略解码-&gt;每次结果不同：诗句生成 集束搜索解码-&gt;每次结果相同，同时接近全局最优解 每次选择概率最大到BeamWidth个序列 将解码出到文字循环输入回到RNN中 直到终止符终止文本分类，文本分词，文本生成整个过程","categories":[],"tags":[]},{"title":"贝塔分布","slug":"贝塔分布","date":"2019-07-15T07:28:18.000Z","updated":"2019-07-16T09:03:30.574Z","comments":true,"path":"2019/07/15/贝塔分布/","link":"","permalink":"https://yongqiangyue.github.io/2019/07/15/贝塔分布/","excerpt":"","text":"贝塔分布，简称${B}$分布，是指一组定义在$(0, 1)$区间的连续概率分布。有两个参数，$\\alpha, \\beta &gt; 0$- $\\Gamma$函数和$\\Gamma$分布 $gamma$函数：$\\Gamma(\\alpha) = \\int_{0}^{\\infty}x^{\\alpha-1}e^{-x}dx$ $gamma$函数的性质：$\\Gamma(\\alpha+1) = \\alpha \\Gamma(\\alpha)$ $gamma$分布的密度函数：$p(x)= \\frac{\\lambda^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\lambda x}, x\\geq0$ 总结：$\\Gamma$函数是阶乘在实数上的拓展 与$B$函数的关系：$B(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$ 贝塔分布的密度函数:$p(x)=\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}x^{a-1}(1-x)^{b-1}, 0&lt;x&lt;1$ 贝塔分布的期望：$E(x) = \\frac{a}{a+b}$","categories":[{"name":"数学","slug":"数学","permalink":"https://yongqiangyue.github.io/categories/数学/"}],"tags":[{"name":"概率","slug":"概率","permalink":"https://yongqiangyue.github.io/tags/概率/"}]},{"title":"梯度下降","slug":"梯度下降-2","date":"2019-07-15T03:55:23.000Z","updated":"2019-07-15T05:33:36.891Z","comments":true,"path":"2019/07/15/梯度下降-2/","link":"","permalink":"https://yongqiangyue.github.io/2019/07/15/梯度下降-2/","excerpt":"","text":"假设函数$$h_{\\theta}(x) = {\\theta}^{\\top}x$$ 损失函数$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m(h(x_i)-y_i)^2$$ BGD，批梯度下降 我们常用的梯度下降都是指批梯度下降。 对$\\theta$求偏导数$$\\frac{\\partial J(\\theta)}{\\partial {\\theta_j}}=\\frac{1}{m}\\sum_{i=1}^m(h(x_i)-y_i)x_i^j$$ 沿着梯度的反方向对${\\theta}$进行迭代更新$${\\theta_j} = {\\theta_j} - \\frac{1}{m}\\sum_{i=1}^m(h(x_i)-y_i)x_i^j$$ 使用所有的样本来计算梯度 SGD，随机梯度下降 随机选择一个样本来近似代表所有样本来计算梯度 由于SGD每次迭代都是随机选择一个样本，故也叫oneline learnig 遇到噪声容易陷入局部最优解 mini-batch SGD 小批量采样来近似代表所有的样本 小批量采样中每个样本计算梯度，然后求均值。作为最终的梯度来更新参数","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://yongqiangyue.github.io/categories/机器学习/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://yongqiangyue.github.io/tags/算法/"}]},{"title":"参数估计","slug":"参数估计","date":"2019-07-12T02:58:31.000Z","updated":"2019-07-12T02:59:29.386Z","comments":true,"path":"2019/07/12/参数估计/","link":"","permalink":"https://yongqiangyue.github.io/2019/07/12/参数估计/","excerpt":"","text":"需要整理 参数估计 假设检验","categories":[{"name":"数学","slug":"数学","permalink":"https://yongqiangyue.github.io/categories/数学/"}],"tags":[{"name":"概率","slug":"概率","permalink":"https://yongqiangyue.github.io/tags/概率/"}]},{"title":"折腾博客","slug":"折腾博客","date":"2019-07-10T11:18:13.000Z","updated":"2019-07-10T13:34:49.891Z","comments":true,"path":"2019/07/10/折腾博客/","link":"","permalink":"https://yongqiangyue.github.io/2019/07/10/折腾博客/","excerpt":"","text":"折腾博客 来由 学习人工智能课程时发现需要很多记录下，想想自己cnblog很久没有更新了。加上目前学习大过程中一直在用juypter notebook做笔记。 在学习小组中很多学友会遇到很多相似的问题，每次回复起来很不方便。 机器学习的算法需要反复的学习和练习。基于上面的三个原因，决定折腾下博客。发现了hexo! hexo安装部署 借助github、hexo、hexoclient搭建 HexoClient使用帮助 个人配置 头像，昵称，支付二维码等 搜索：npm install hexo-generator-json-content –save 公告：增加一个机器学习的二维码 个人介绍","categories":[{"name":"程序员","slug":"程序员","permalink":"https://yongqiangyue.github.io/categories/程序员/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://yongqiangyue.github.io/tags/hexo/"}]}]}